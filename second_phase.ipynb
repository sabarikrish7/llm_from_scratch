{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462cc4a4-91e2-42b4-a20b-e7505890c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your      (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey   (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts    (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with      (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one       (x^5)\n",
    "        [0.05, 0.80, 0.55],  # step      (x^6)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15975c92-528d-4fed-8dfc-f36a36428fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1=inputs[0]\n",
    "input_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f544e5-d120-4f84-9cd8-b48564cc83a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_query=inputs[1]\n",
    "input_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b650c4-5e50-4707-9a4e-5e49bc5c96ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(input_query,input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a2f6fb-1239-40e7-89a2-a66a39150f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4300)\n",
      "tensor(0.1500)\n",
      "tensor(0.8900)\n"
     ]
    }
   ],
   "source": [
    "for ele in inputs[0]:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "440e2be3-7287-46d6-b609-eea0e561ba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4300)\n",
      "tensor(0.1500)\n",
      "tensor(0.8900)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index,ele in enumerate(inputs[0]):\n",
    "    print(inputs[0][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2942bfee-628b-42f5-bed3-85cce6a9d0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "res=0\n",
    "i=0\n",
    "for index,ele in enumerate(inputs[i]):\n",
    "    res+=(inputs[i][index]*input_query[index])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b1147fd-4f23-456f-bfab-2088ae047f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query=inputs[1]#2nd i/p token is the query token\n",
    "attn_scores_2=torch.empty(inputs.shape[0])\n",
    "for i,x_i in enumerate(inputs):\n",
    "    attn_scores_2[i]=torch.dot(x_i,input_query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2413798-0887-46bb-8417-bbac9700ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2=torch.softmax(attn_scores_2,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a568abcb-f2f0-4fa0-9ce0-7c2af2f2f0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13854756951332092-->tensor([0.4300, 0.1500, 0.8900])\n",
      "0.2378913015127182-->tensor([0.5500, 0.8700, 0.6600])\n",
      "0.23327402770519257-->tensor([0.5700, 0.8500, 0.6400])\n",
      "0.12399158626794815-->tensor([0.2200, 0.5800, 0.3300])\n",
      "0.10818186402320862-->tensor([0.7700, 0.2500, 0.1000])\n",
      "0.15811361372470856-->tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    }
   ],
   "source": [
    "query=inputs[1]\n",
    "context_vec2=torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    print(f\"{attn_weights_2[i]}-->{inputs[i]}\")\n",
    "    #context_vec2+=attn_weights_2[i]*x_i\n",
    "#print(context_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f36093db-bd99-4a2c-a81a-9888f2f2318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query=inputs[1]\n",
    "context_vec2=torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec2+=attn_weights_2[i]*x_i\n",
    "print(context_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb2407-25ea-4836-be02-ffa525937314",
   "metadata": {},
   "source": [
    "generalising simple self attention mech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb06fddd-0001-4956-b94a-a9647cd7e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores=torch.empty(6,6)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    for j,x_j in enumerate(inputs):\n",
    "        attn_scores[i][j]=torch.dot(x_i,x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93a8d20b-8c6c-439d-9436-e7ab26a619f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in pytorch for loops are slower as compared to matrix multiplications\n",
    "attn_scores=inputs@inputs.T\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1cbef52-f500-490f-ae32-271f163eb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights=torch.softmax(attn_scores,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96f49c22-c287-49bd-b458-5966c29f04f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_context_vecs=attn_weights@inputs\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b6414a-3c7e-4891-8115-dbcf5198fb76",
   "metadata": {},
   "source": [
    "Implementing self attention weights step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c3bb5-68f2-4214-9cf0-830287d73e02",
   "metadata": {},
   "source": [
    "Computing the attention weights step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8273d2af-9b23-44db-a163-690b48f3d64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2=inputs[1]\n",
    "d_in=inputs.shape[1]\n",
    "d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "393f37cb-da8f-4672-8548-3664567d32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query=torch.nn.Parameter(torch.rand(d_in,d_out))\n",
    "W_key=torch.nn.Parameter(torch.rand(d_in,d_out))\n",
    "W_value=torch.nn.Parameter(torch.rand(d_in,d_out))\n",
    "#we use torch.nn.Parameter to make sure that the matrices or tensors\n",
    "#are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a3ae45d-c0f2-4f71-be9d-3a0c689f24f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2=x_2@W_query\n",
    "query_2#the same query vector is reused everywhere\n",
    "#for key and value vectors every vector has diff key and value vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3604b57-6eeb-4ba2-a033-3f7bb8b4b8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3669, 0.7646],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.4361, 1.1156],\n",
       "        [0.2408, 0.6706],\n",
       "        [0.1827, 0.3292],\n",
       "        [0.3275, 0.9642]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys=inputs@W_key\n",
    "value=inputs@W_value\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc628a7c-0850-4b74-b55f-8e582b5d36d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_2=keys[1]\n",
    "attn_score_22=torch.dot(keys_2,query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "158af21c-6bc4-4e55-98a5-fff6526dca35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2= query_2@keys.T\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc5011b5-2730-4c08-ae11-5ec76247120a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k=keys.shape[1]\n",
    "attn_weights_2=torch.softmax(attn_scores_2/d_k**0.5,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e84b190-bc3e-4b7e-b2bc-8f4f0a52b8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3069, 0.8188], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2=attn_weights_2@value\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa9232-8298-44f8-9aa0-ffb4c15e2805",
   "metadata": {},
   "source": [
    "Generalising for all vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e78990fa-a7da-40fe-9ed5-50da48c52225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, d_in)\n",
    "        queries = x @ self.W_query\n",
    "        keys    = x @ self.W_key\n",
    "        values  = x @ self.W_value\n",
    "\n",
    "        attn_scores  = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / (keys.shape[-1] ** 0.5),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "# Example usage\n",
    "torch.manual_seed(123)\n",
    "\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "sa_v1(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f58f014a-8338-448b-b198-f864468724e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77e3b631-14c1-459c-92cd-5dbb50b21683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, d_in)\n",
    "        queries = self.W_query(x)\n",
    "        keys    = self.W_key(x)\n",
    "        values  = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        d_k = keys.shape[-1]\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / (d_k ** 0.5),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "# Example usage\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "sa_v2(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd78260-7f7b-4233-897c-e143d2c45859",
   "metadata": {},
   "source": [
    "3.5 Hiding Causal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23070a5d-400a-44d0-93ae-bc5ea5f379e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
       "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
       "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys    = sa_v2.W_key(inputs)\n",
    "values  = sa_v2.W_value(inputs)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "d_k = keys.shape[-1]\n",
    "\n",
    "attn_weights = torch.softmax(\n",
    "    attn_scores / (d_k ** 0.5),\n",
    "    dim=-1\n",
    ")\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b364284-50a2-4aa2-8c84-cc4813a1197a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length=attn_scores.shape[0]\n",
    "mask_simple=torch.tril(torch.ones(context_length,context_length))\n",
    "mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "177a4367-7945-4b76-96c6-76e685d57a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple=attn_weights*mask_simple\n",
    "masked_simple\n",
    "#but now the rows do not sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77b2926a-1323-40e1-87b0-74af6ee5a4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums=masked_simple.sum(dim=-1,keepdim=True)\n",
    "masked_simple_norm=masked_simple/row_sums\n",
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3131f27-8bb1-4f1c-92ca-1fe65757f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###instead of doing these 4 steps u have an alternate method\n",
    "###just do the masking just after receiving the attn scores and before converting to attn weights\n",
    "###also mask with -inf so that when u do torch.exp it becomes 0\n",
    "\"\"\"mask = torch.triu(\n",
    "    torch.ones(context_length, context_length),\n",
    "    diagonal=1\n",
    ")\n",
    "\n",
    "# Apply mask\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "\n",
    "print(masked)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891c098-7f1e-44b9-b6a8-5fbf6178632e",
   "metadata": {},
   "source": [
    "Dropout masks dooes masking on top of causal masks \n",
    "This is done while training to reduce overfitting\n",
    "##modern llms dont use it gpt2 used it however"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c112ba9-fc35-4a03-93f2-1b495c92b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "layer=torch.nn.Dropout(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40418b4f-0ff3-4783-bee1-57a5b631c913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=torch.ones(6,6)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0fd1d541-487f-4386-867d-59a1bd2c6778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2., 2.],\n",
       "        [0., 2., 0., 0., 0., 0.],\n",
       "        [0., 0., 2., 0., 2., 0.],\n",
       "        [2., 2., 0., 0., 0., 2.],\n",
       "        [2., 0., 0., 0., 0., 2.],\n",
       "        [0., 2., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc88008-8024-45fd-91f2-3a5f51b02a37",
   "metadata": {},
   "source": [
    "#to maintain approximately the same sum in each row \n",
    "#we rescale other values by 1/1-d.r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e899a6-451e-4ec2-a344-6c9109850d3b",
   "metadata": {},
   "source": [
    "Implementing a compact casual self attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b61e50a-3d08-488b-8ec3-1fe6b8bf6e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch=torch.stack((inputs,inputs),dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "055599b5-f178-427d-a161-97287c2b7895",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_length, dropout,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout=torch.nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "    \"mask\",\n",
    "    torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    ")\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, d_in)\n",
    "        b,num_tokens,d_in=x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys    = self.W_key(x)\n",
    "        values  = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)  # Changed transpose\n",
    "\n",
    "        attn_scores.masked_fill_(\n",
    "                self.mask.bool()[:num_tokens, :num_tokens],\n",
    "                -torch.inf\n",
    "        )  # New, ops are in-place\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1] ** 0.5,\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        attn_weights = self.dropout(attn_weights)  # New\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "torch.manual_seed(789)\n",
    "ca = CausalAttention(d_in, d_out,context_length=batch.shape[1],dropout=0.0)\n",
    "\n",
    "ca(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c6110-0c64-4c1a-bde1-129d348e82eb",
   "metadata": {},
   "source": [
    "Extending single head attention to multi head attention\n",
    "We use multihead attention so that we can have various views for the same i/p vector ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c5e4746f-441a-47a0-ac25-6fb94297cb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_head=2,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([CausalAttention(d_in,d_out,context_length,dropout,qkv_bias) for _ in range(num_head)\n",
    "        ])\n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x) for head in self.heads],dim=-1)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3 , 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, dropout=0.0, num_head=2\n",
    ")\n",
    "mha(batch)\n",
    "##notice here since we call for heads using a for loop we waste a lot of time \n",
    "##even though each head is run independently\n",
    "##so we will see a diff implementation to save time and utilise GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6e2f7-450d-420c-8c40-489ad7e9f6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
